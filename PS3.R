###########Problem Set 3############
#Ansel Schiavone
library(invgamma)
library(coda)
library(TeachingDemos)
library(rstan)
setwd("C:/Users/Ansel/Box Sync/Documents/Bayesian statistics/Problem sets")
################ Problem 1.c (Analytical)###########
#create P matrix
P <- matrix(c(0, 0.15, 0.3, 0.5, 0 , 0.7, 0.5, 0.85, 0), 3, 3, byrow=TRUE)

#calculate eigenvalues/vectors
ev <- eigen(P)
values<-ev$values
vectors <-ev$vectors

#calculate scalar vector for eigenvector that corresponds to maximum eigenvalue
scale_vector = rep(sum(vectors[,which.max(values)]), times=3)

#calculate stationary ergodic distribution of system
stationary_dist = vectors[,which.max(values)]/scale_vector

rm(list=ls())
############### Problem 1 #########

##Setup:
set.seed(123)
n.sim <- 5000
x <- rnorm(100,25,1.5)
n <- 100
mu <- vector()
sigma <-vector()

#assign arbitrary hyperparmeter values
gamma <- 1  
tau <- 100
a <-0.1
b<-0.1

#initial values:
mu[1]<-15
sigma[1]<- 1

#We know from class lecture on conjugate analysis for the normal model that:
#conjugate prior: mu|sigma,x~N(m,t^2)
#conjugate prior: sigma~IG(alpha,beta)
for(i in 2:n.sim){
  # sample mu|sigma,x
  p1 <-  sum(x)/sigma[i-1]+gamma/tau
  p2 <-  n/sigma[i-1]+1/tau
  mu[i]<-rnorm(n=1,p1/p2,1/sqrt(p2))
  
  # sample sigma|mu,x
  alpha  <- a+n/2
  beta  <- b+sum((x-mu[i])^2)/2
  sigma[i]<-rinvgamma(1, alpha, beta)
}

##1.a)

#trace plots:
burn<-100
plot(c(burn:n.sim),mu[burn:n.sim], type="l", ylab=expression(mu),xlab="iteration")
plot(c(burn:n.sim),sigma[burn:n.sim], type="l", ylab=expression(sigma),xlab="iteration")

#autocorrelation plots:
acf(mu,ylab="")
acf(sigma,ylab="")

#Geweke statistics (using coda package):
geweke.diag(mu)
geweke.plot(as.mcmc(mu))
geweke.diag(sigma)
geweke.plot(as.mcmc(sigma))


#Based on the trace plots, it appears the Gibbs sampler converges quickly
#to a finite value for both mu and sigma. The autocorrelation plots indicate
#only a small level of autocorrelation that does not appear to be problematic.
#Finally, the Z-scores generated by the Geweke statistic both fall between the
#[-2,2] range, indicating efficient convergence.

##1.b)

#Mu posterior distribution
hist(mu,prob=T,breaks=100,xlim=c(24.5,26),xlab=expression(mu))
mu_hpd=emp.hpd(mu,conf = 0.95)
abline(v=mu_hpd[1], lty=2)
abline(v=mu_hpd[2], lty=2)

#Sigma posterior distribution
hist(sigma,prob=T,breaks=100,xlim=c(1,3),xlab=expression(sigma))
sigma_hpd=emp.hpd(sigma,conf = 0.95)
abline(v=sigma_hpd[1], lty=2)
abline(v=sigma_hpd[2], lty=2)

##1.c)
options(mc.cores = parallel::detectCores())

modelString = "
  data{
    int<lower=0> n;
    real x[n];
  }
  parameters{
    real mu;
    real<lower=0> sigmasq;
  }
  transformed parameters{
    real<lower=0> sigma;
    sigma = sqrt(sigmasq);
  }
  model{
    x~normal(mu, sigma);
    mu~normal(0,100);
    sigmasq~inv_gamma(0.1,0.1);
  }

" 
dataList = list(x = x , n = n)
stanFit = stan(model_code=modelString , data=dataList ,
               chains=4 , iter=10000 , warmup=200 , thin=1,
               init=function(){list(mu=mean(x), sd=sd(x))})

# print stan output
print(stanFit)
print(stanFit, digits_summary=4)

# plot stan output
traceplot(stanFit,inc_warmup = T)
plot(stanFit)

#Yes, outcomes lok the same
rm(list=ls())

############### Problem 2 #########
library(LearnBayes)
#2.a
data(darwin)
series<-darwin$difference

#2.b
par_tune = 12 #tuning parameter

lfx<-function(x,m,s){
  dcauchy(x,m,s,log=T)
}

likelihood <- function(params){
  m = params[1]
  s = params[2]
  log.likelihood = lfx(series,m,s)
  sum.ll = sum(log.likelihood)
  return(sum.ll)   
}

prior <- function(params){
  sigma = params[2]
  return(log(1/sigma))
}

posterior <- function(params){
  likelihood(params) + prior(params)
}

proposalfunction <- function(params){
  mu <- rnorm(1,mean = params[1], par_tune)
  sd <- rgamma(1,params[1]/par_tune, scale = par_tune)
  return(c(mu, sd))
}

MH.sampler <- function(initial, nsamp){
  chain = array(dim = c(nsamp+1,2))
  chain[1,] = initial
  for (i in 1:(nsamp)){
    proposal = proposalfunction(chain[i,])
    probab = exp(posterior(proposal) - posterior(chain[i,]))
    if(runif(1) < probab){ 
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}

initial <- c(mean(series),sd(series)) #assign initial conditions equal to destriptive statistics of data (mean and sd)
nsamp <- 10000
MCMC.out <- MH.sampler(initial=initial, nsamp=nsamp)


#2.c)

# Burn-in
burnIn <- 2000
MCMC.burn <- data.frame(MCMC.out[-(1:burnIn),])
colnames(MCMC.burn) <- c("mu","sigma")


# Traceplots for MCMCM 
plot(MCMC.burn$mu,type="l",ylab="")
abline(h=mean(MCMC.burn$mu),col=2,lwd=2)
plot(MCMC.burn$sigma/par_tune,type="l",ylab="")
abline(h=mean(MCMC.burn$sigma/par_tune),col=2,lwd=2)


# ACF Plot for MCMCM 
acf(MCMC.burn$mu,ylab="")
acf(MCMC.burn$sigma,ylab="")

#Geweke statistics (using coda package):
geweke.diag(MCMC.burn$mu)
geweke.plot(as.mcmc(MCMC.burn$mu))
geweke.diag(MCMC.burn$sigma)
geweke.plot(as.mcmc(MCMC.burn$sigma))

#Based on the trace plots, it appears the MH sampler converges
#to a finite value for both mu and sigma. The autocorrelation plots indicate
#displays high autocorrelation at low lags, but monotonically decreasing as lags increase
#Finally, the Z-scores generated by the Geweke statistic both fall between the
#[-2,2] range, indicating efficient convergence.

#2.d)
#Mu posterior distribution
hist(MCMC.burn$mu,prob=T,breaks=100,xlab=expression(mu))
mu_hpd=emp.hpd(MCMC.burn$mu,conf = 0.95)
abline(v=mu_hpd[1], lty=2)
abline(v=mu_hpd[2], lty=2)

#sigma posterior distribution
hist(MCMC.burn$sigma/par_tune,prob=T,breaks=100,xlab=expression(sigma))
sigma_hpd=emp.hpd(MCMC.burn$sigma/par_tune,conf = 0.95)
abline(v=sigma_hpd[1], lty=2)
abline(v=sigma_hpd[2], lty=2)

#2.e)
options(mc.cores = parallel::detectCores()) #four core processing

#define MH model in STAN
modelString = "
  data {
    int<lower=0> n ;
    real x[n] ; 
  }
  parameters {
    real mu ; 
    real<lower=0> sigma ;
  } 
  model {
    mu ~ normal(22, 13);
    sigma  ~ gamma(22,13) ;
    x  ~ cauchy(mu, sigma) ;
  }
" 
#run model in STAN
dataList = list(x = series , n = length(series))
stanFit = stan(model_code=modelString , data=dataList ,
               chains=4 , iter=10000 , warmup=200 , thin=1,
               init=function(){list(mu=mean(series), sigma=sd(series))})

#plot traceplots and histograms generated by STAN
traceplot(stanFit)  
stan_hist(stanFit)

#outcomes from STAN model appear to be consistent with those from the constructed
#Metropolis-Hasting model.
